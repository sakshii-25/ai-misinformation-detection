# -*- coding: utf-8 -*-
"""Sakshi's_Final_dissertation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vdfVaucDuqMU403C1l-XeHKMCNJUe2zf
"""

import pandas as pd
df = pd.read_csv('data.csv')
df.head()

df.describe()

df = df.dropna()

print(df.info())
#displaying all the columns data type ,null or not count

print(df.shape)

import re
from nltk.corpus import stopwords
!pip install nltk
import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

def preprocess_text(tweet):
    tweet = re.sub(r'http\S+', '', tweet)  # Remove URLs
    tweet = re.sub(r'\W', ' ', tweet)  # Remove special characters
    tweet = tweet.lower()  # Convert to lowercase
    print(tweet)
    tokens = word_tokenize(tweet)
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stopwords.words('english')]
    return ' '.join(tokens)
    print(tokens)
df['clean_tweet'] = df['tweet'].apply(preprocess_text)
df.head()

df['clean_tweet'] = df['clean_tweet'].apply(lambda tweet: re.sub(r'[^\w\s]', '', tweet))  # Remove punctuation from 'clean_tweet' column

def remove_numbers(tweet):
    return re.sub(r'\d+', '', tweet)  # Remove numbers

df['clean_tweet'] = df['clean_tweet'].apply(remove_numbers) # Apply the function to the 'clean_tweet' column of the DataFrame

from bs4 import BeautifulSoup

def remove_html(text):
    return BeautifulSoup(text, "html.parser").get_text()

df['clean_tweet'] = df['clean_tweet'].apply(remove_html)

def remove_emojis(text):
    return text.encode('ascii', 'ignore').decode('ascii')

df['clean_tweet'] = df['clean_tweet'].apply(remove_emojis)

!pip install contractions # Install the contractions library
import contractions

def fix_contractions(tweet):
    return contractions.fix(tweet)

df['clean_tweet'] = df['clean_tweet'].apply(fix_contractions) # Apply the function to the DataFrame

df['clean_tweet'] = df['clean_tweet'].apply(lambda tweet: re.sub(r'\s+', ' ', tweet).strip())  # Remove extra whitespaces

print(df.BinaryNumTarget.value_counts())

# Check if 'clean_tweet' exists
print('df columns:', df.columns)

print(df.info())

print(df.shape)

import pandas as pd

# Select only numeric columns from the DataFrame
numeric_df = df.select_dtypes(include=[float, int])

# Handle missing values by filling them with 0 or any other appropriate value
numeric_df = numeric_df.fillna(0)

# Calculate the correlation matrix
correlation_matrix = numeric_df.corr()

# Print the correlation matrix
print(correlation_matrix)

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Assuming 'correlation_matrix' is your correlation matrix
plt.figure(figsize=(20, 18))

# Round the correlation values to 2 decimal places for better readability
rounded_correlation_matrix = np.round(correlation_matrix, 2)

# Create the heatmap with rounded values
heatmap = sns.heatmap(
    rounded_correlation_matrix,
    annot=True,
    fmt='.2f',
    cmap='coolwarm',
    vmin=-1,
    vmax=1,
    annot_kws={"size": 8}  # Adjust font size for better readability
)

plt.title('Correlation Matrix Heatmap')
plt.show()

# Select upper triangle of correlation matrix
upper = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))

# Find index of feature columns with correlation greater than 0.8
high_corr_features = [column for column in upper.columns if any(upper[column] > 0.8)]
print("Highly correlated features:", high_corr_features)

from sklearn.feature_extraction.text import TfidfVectorizer

# Apply TF-IDF vectorization
tfidf = TfidfVectorizer(max_features=10000, ngram_range=(1, 2))  # Bigram and unigram
X_tfidf = tfidf.fit_transform(df['clean_tweet'])

print("TF-IDF shape:", X_tfidf.shape)

print(X_tfidf)

print(X_tfidf.shape)

from wordcloud import WordCloud
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# Assuming X_tfidf and df have the same number of rows
# Check the shape of X_tfidf to avoid index out of range errors
print("Shape of X_tfidf:", X_tfidf.shape)
print("Number of rows in df:", len(df))

# Make sure the indices align with the length of X_tfidf
real_news_indices = df[df['BinaryNumTarget'] == 1].index.intersection(range(X_tfidf.shape[0]))  # 1 represents real news
fake_news_indices = df[df['BinaryNumTarget'] == 0].index.intersection(range(X_tfidf.shape[0]))  # 0 represents fake news

# Get the corresponding TF-IDF features for real and fake news
real_tfidf = X_tfidf[real_news_indices].toarray()
fake_tfidf = X_tfidf[fake_news_indices].toarray()

# Get the feature names (i.e., words) from the TF-IDF vectorizer
words = tfidf.get_feature_names_out()

# Sum the TF-IDF scores for each word in real and fake news
real_word_scores = np.sum(real_tfidf, axis=0)
fake_word_scores = np.sum(fake_tfidf, axis=0)

# Create a dictionary of words and their corresponding TF-IDF scores for real and fake news
real_word_dict = dict(zip(words, real_word_scores))
fake_word_dict = dict(zip(words, fake_word_scores))

# Generate word cloud for real news
real_wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(real_word_dict)

# Generate word cloud for fake news
fake_wordcloud = WordCloud(width=800, height=400, background_color='black').generate_from_frequencies(fake_word_dict)

# Plot the word clouds
plt.figure(figsize=(16, 8))

# Real news word cloud
plt.subplot(1, 2, 1)
plt.imshow(real_wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud for Real News', fontsize=20)

# Fake news word cloud
plt.subplot(1, 2, 2)
plt.imshow(fake_wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud for Fake News', fontsize=20)

plt.show()

from sklearn.decomposition import TruncatedSVD

# Apply LSA (SVD) to reduce the dimensionality of TF-IDF matrix
lsa = TruncatedSVD(n_components=300, random_state=42)  # Reduce to 300 dimensions
X_lsa = lsa.fit_transform(X_tfidf)

print("LSA-transformed shape:", X_lsa.shape)

from gensim.models import Word2Vec
import nltk

# Tokenize the clean_tweet text
tokenized_tweets = [nltk.word_tokenize(tweet) for tweet in df['clean_tweet']]

# Train Word2Vec model on tokenized tweets
w2v_model = Word2Vec(sentences=tokenized_tweets, vector_size=100, window=5, min_count=2)

# Function to get the average Word2Vec embedding for a tweet
def get_avg_w2v(tweet_tokens, model, vector_size):
    vectors = [model.wv[word] for word in tweet_tokens if word in model.wv]
    if len(vectors) > 0:
        return np.mean(vectors, axis=0)
    else:
        return np.zeros(vector_size)

# Apply Word2Vec to the entire dataset to get embeddings for each tweet
X_w2v = np.array([get_avg_w2v(tweet, w2v_model, 100) for tweet in tokenized_tweets])

print("Word2Vec embeddings shape:", X_w2v.shape)

import numpy as np

# Combine the LSA-reduced TF-IDF features with Word2Vec embeddings
X_combined = np.hstack([X_lsa, X_w2v])

print("Combined features shape:", X_combined.shape)

# Display the first few rows of potential target columns to identify the correct one
potential_targets = ['5_label_majority_answer', '3_label_majority_answer', 'BinaryNumTarget', 'BotScoreBinary']
print(df[potential_targets].head())

from sklearn.model_selection import train_test_split

# Assuming 'BinaryNumTarget' is your target variable
y = df['BinaryNumTarget']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)

print("Training set shape:", X_train.shape)
print("Test set shape:", X_test.shape)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report

# Initialize Logistic Regression model
model_lr = LogisticRegression(max_iter=1000)

# Train the model on training data
model_lr.fit(X_train, y_train)

# Predict on the training set
y_train_pred = model_lr.predict(X_train)

# Predict on the test set
y_test_pred = model_lr.predict(X_test)

# Print classification report for training data
print("Logistic Regression-Training Data Evaluation:")
print(classification_report(y_train, y_train_pred))

# Print classification report for test data
print("Logistic Regression-Test Data Evaluation:")
print(classification_report(y_test, y_test_pred))

import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import seaborn as sns

# Function to plot confusion matrix
def plot_confusion_matrix(y_true, y_pred, title):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
    plt.title(title)
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.show()

# Confusion matrix for training data
y_train_pred = model_lr.predict(X_train)
plot_confusion_matrix(y_train, y_train_pred, 'Confusion Matrix - Training Data')

# Confusion matrix for test data
y_test_pred = model_lr.predict(X_test)
plot_confusion_matrix(y_test, y_test_pred, 'Confusion Matrix - Test Data')

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

# Initialize the Random Forest classifier with regularization (limiting depth, min samples)
rf_model = RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_split=10, min_samples_leaf=5, random_state=42)

# Train the Random Forest model
rf_model.fit(X_train, y_train)

# Predictions on the training set
y_train_pred_rf = rf_model.predict(X_train)

# Predictions on the test set
y_test_pred_rf = rf_model.predict(X_test)

# Print classification report for training data
print("Random Forest (with Regularization) - Training Data Evaluation:")
print(classification_report(y_train, y_train_pred_rf))

# Print classification report for test data
print("Random Forest (with Regularization) - Test Data Evaluation:")
print(classification_report(y_test, y_test_pred_rf))

# Plot confusion matrix for training data
plot_confusion_matrix(y_train, y_train_pred_rf, 'Random Forest Confusion Matrix - Training Data')

# Plot confusion matrix for test data
plot_confusion_matrix(y_test, y_test_pred_rf, 'Random Forest Confusion Matrix - Test Data')

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report

# Initialize the KNN classifier with k neighbors (start with k=5)
knn_model = KNeighborsClassifier(n_neighbors=5)

# Train the KNN model
knn_model.fit(X_train, y_train)

# Predictions on the training set
y_train_pred_knn = knn_model.predict(X_train)

# Predictions on the test set
y_test_pred_knn = knn_model.predict(X_test)

# Print classification report for training data
print("KNN - Training Data Evaluation:")
print(classification_report(y_train, y_train_pred_knn))

# Print classification report for test data
print("KNN - Test Data Evaluation:")
print(classification_report(y_test, y_test_pred_knn))

# Plot confusion matrix for training data
plot_confusion_matrix(y_train, y_train_pred_knn, 'KNN Confusion Matrix - Training Data')

# Plot confusion matrix for test data
plot_confusion_matrix(y_test, y_test_pred_knn, 'KNN Confusion Matrix - Test Data')

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report

# Initialize the Decision Tree classifier with regularization (limiting depth and minimum samples)
dt_model = DecisionTreeClassifier(max_depth=10, min_samples_split=10, min_samples_leaf=5, random_state=42)

# Train the Decision Tree model
dt_model.fit(X_train, y_train)

# Predictions on the training set
y_train_pred_dt = dt_model.predict(X_train)

# Predictions on the test set
y_test_pred_dt = dt_model.predict(X_test)

# Print classification report for training data
print("Decision Tree (with Regularization) - Training Data Evaluation:")
print(classification_report(y_train, y_train_pred_dt))

# Print classification report for test data
print("Decision Tree (with Regularization) - Test Data Evaluation:")
print(classification_report(y_test, y_test_pred_dt))

# Plot confusion matrix for training data
plot_confusion_matrix(y_train, y_train_pred_dt, 'Decision Tree Confusion Matrix - Training Data')

# Plot confusion matrix for test data
plot_confusion_matrix(y_test, y_test_pred_dt, 'Decision Tree Confusion Matrix - Test Data')

import xgboost as xgb
from sklearn.metrics import classification_report

# Initialize the XGBoost classifier
xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)

# Train the XGBoost model
xgb_model.fit(X_train, y_train)

# Predictions on the training set
y_train_pred_xgb = xgb_model.predict(X_train)

# Predictions on the test set
y_test_pred_xgb = xgb_model.predict(X_test)

# Print classification report for training data
print("XGBoost - Training Data Evaluation:")
print(classification_report(y_train, y_train_pred_xgb))

# Print classification report for test data
print("XGBoost - Test Data Evaluation:")
print(classification_report(y_test, y_test_pred_xgb))

# Plot confusion matrix for training data
plot_confusion_matrix(y_train, y_train_pred_xgb, 'XGBoost Confusion Matrix - Training Data')

# Plot confusion matrix for test data
plot_confusion_matrix(y_test, y_test_pred_xgb, 'XGBoost Confusion Matrix - Test Data')

import pandas as pd
import matplotlib.pyplot as plt

# Create a dictionary to store the results
results = {
    'Algorithm': ['Logistic Regression', 'Random Forest', 'KNN', 'Decision Tree', 'XGBoost'],
    'Training Accuracy': [0.93, 0.92, 0.97, 0.88, 0.98],
    'Test Accuracy': [0.93, 0.91, 0.96, 0.86, 0.95],
    'Test Precision': [0.93, 0.91, 0.96, 0.86, 0.95],
    'Test Recall': [0.93, 0.91, 0.96, 0.86, 0.96],
    'Test F1-Score': [0.93, 0.91, 0.96, 0.86, 0.95]
}

# Convert the results dictionary into a DataFrame
results_df = pd.DataFrame(results)

# Print the comparison table
print("Comparison of Algorithms:")
print(results_df)

import matplotlib.pyplot as plt

# Plot a bar chart to compare the accuracy of the models
plt.figure(figsize=(10, 6))
plt.bar(results_df['Algorithm'], results_df['Test Accuracy'], color='skyblue')
plt.title('Test Accuracy Comparison of Algorithms')
plt.xlabel('Algorithm')
plt.ylabel('Test Accuracy')
plt.ylim([0.8, 1.0])  # Set the y-axis limits for better visualization
plt.xticks(rotation=45)
plt.show()

# Define colors for each metric
colors = {'Test Precision': 'orange', 'Test Recall': 'lightcoral', 'Test F1-Score': 'lightgreen'}

# Plot bar charts for Precision, Recall, and F1-Score with different colors
metrics = ['Test Precision', 'Test Recall', 'Test F1-Score']

for metric in metrics:
    plt.figure(figsize=(10, 6))
    plt.bar(results_df['Algorithm'], results_df[metric], color=colors[metric])
    plt.title(f'{metric} Comparison of Algorithms')
    plt.xlabel('Algorithm')
    plt.ylabel(metric)
    plt.ylim([0.8, 1.0])  # Set the y-axis limits for better visualization
    plt.xticks(rotation=45)
    plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Assuming you already have these metrics calculated for the XGBoost model
metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']


xgboost_metrics = [0.95, 0.95, 0.96, 0.95]

# Create a bar graph for XGBoost metrics
plt.figure(figsize=(10, 6))
bars = plt.bar(metrics, xgboost_metrics, color=['skyblue', 'lightgreen', 'lightcoral', 'gold'])

# Annotate the bars with their values
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval, round(yval, 2), ha='center', va='bottom')

plt.title('XGBoost Model Evaluation Metrics')
plt.ylim([0.8, 1.0])  # Adjust the y-axis for better visualization
plt.ylabel('Score')
plt.xlabel('Evaluation Metrics')

plt.show()

from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt

# Assuming the models are already trained (Logistic Regression, Random Forest, KNN, Decision Tree, XGBoost)

# Store the models in a dictionary for easier handling
models = {
    'Logistic Regression': model_lr,  # Assuming you stored the trained Logistic Regression model as model_lr
    'Random Forest': rf_model,        # Random Forest model
    'KNN': knn_model,                 # KNN model
    'Decision Tree': dt_model,        # Decision Tree model
    'XGBoost': xgb_model             # XGBoost model
}

# Plot ROC curves for each model
plt.figure(figsize=(10, 8))

for model_name, model in models.items():
    # Get the predicted probabilities (for the positive class)
    y_test_prob = model.predict_proba(X_test)[:, 1]

    # Compute ROC curve and ROC AUC
    fpr, tpr, _ = roc_curve(y_test, y_test_prob)
    auc = roc_auc_score(y_test, y_test_prob)

    # Plot the ROC curve
    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc:.2f})')

# Plot formatting
plt.plot([0, 1], [0, 1], color='navy', linestyle='--')  # Diagonal line for random guessing
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve Comparison of Models')
plt.legend(loc='lower right')
plt.show()

# Example manual input (this is the new tweet/news you want to test)
manual_input = "Breaking news: The new vaccine has been approved and will be distributed worldwide."

# Preprocess the input (same preprocessing steps you applied to your dataset)
def preprocess_manual_input(input_text):
    # Apply the same preprocessing steps: remove URLs, punctuation, stopwords, etc.
    input_text = re.sub(r'http\S+', '', input_text)  # Remove URLs
    input_text = re.sub(r'\W', ' ', input_text)  # Remove special characters
    input_text = input_text.lower()  # Convert to lowercase
    tokens = word_tokenize(input_text)
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stopwords.words('english')]
    return tokens  # Return the tokenized form

# Preprocess the manual input
manual_input_tokens = preprocess_manual_input(manual_input)

# Step 1: Vectorize the manual input using the trained TF-IDF vectorizer
manual_input_tfidf = tfidf.transform([' '.join(manual_input_tokens)])  # Join tokens back into a single string

# Step 2: Apply LSA (TruncatedSVD) to reduce dimensionality using the same SVD model
manual_input_lsa = lsa.transform(manual_input_tfidf)

# Step 3: Get the Word2Vec embedding for the manual input (same approach as during training)
manual_input_w2v = get_avg_w2v(manual_input_tokens, w2v_model, 100)

# Step 4: Combine the LSA-reduced TF-IDF features with Word2Vec embeddings
manual_input_combined = np.hstack([manual_input_lsa, manual_input_w2v.reshape(1, -1)])  # Reshape to match dimensions

# Step 5: Make a prediction using the trained XGBoost model
manual_prediction = xgb_model.predict(manual_input_combined)

# Print the result (1 for real news, 0 for fake news)
if manual_prediction == 1:
    print("The news is REAL.")
else:
    print("The news is FAKE.")

# Example manual input (this is the new tweet/news you want to test)
manual_input = "NASA’s Perseverance rover successfully landed on Mars on February 18, 2021, as part of the agency's mission to search for signs of ancient life on the red planet."

# Preprocess the input (same preprocessing steps you applied to your dataset)
def preprocess_manual_input(input_text):
    # Apply the same preprocessing steps: remove URLs, punctuation, stopwords, etc.
    input_text = re.sub(r'http\S+', '', input_text)  # Remove URLs
    input_text = re.sub(r'\W', ' ', input_text)  # Remove special characters
    input_text = input_text.lower()  # Convert to lowercase
    tokens = word_tokenize(input_text)
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stopwords.words('english')]
    return tokens  # Return the tokenized form

# Preprocess the manual input
manual_input_tokens = preprocess_manual_input(manual_input)

# Step 1: Vectorize the manual input using the trained TF-IDF vectorizer
manual_input_tfidf = tfidf.transform([' '.join(manual_input_tokens)])  # Join tokens back into a single string

# Step 2: Apply LSA (TruncatedSVD) to reduce dimensionality using the same SVD model
manual_input_lsa = lsa.transform(manual_input_tfidf)

# Step 3: Get the Word2Vec embedding for the manual input (same approach as during training)
manual_input_w2v = get_avg_w2v(manual_input_tokens, w2v_model, 100)

# Step 4: Combine the LSA-reduced TF-IDF features with Word2Vec embeddings
manual_input_combined = np.hstack([manual_input_lsa, manual_input_w2v.reshape(1, -1)])  # Reshape to match dimensions

# Step 5: Make a prediction using the trained XGBoost model
manual_prediction = xgb_model.predict(manual_input_combined)

# Print the result (1 for real news, 0 for fake news)
if manual_prediction == 1:
    print("The news is REAL.")
else:
    print("The news is FAKE.")

# Example manual input (this is the new tweet/news you want to test)
manual_input = "The 5G network spreads COVID-19 by weakening people's immune systems and transmitting the virus through radio waves."

# Preprocess the input (same preprocessing steps you applied to your dataset)
def preprocess_manual_input(input_text):
    # Apply the same preprocessing steps: remove URLs, punctuation, stopwords, etc.
    input_text = re.sub(r'http\S+', '', input_text)  # Remove URLs
    input_text = re.sub(r'\W', ' ', input_text)  # Remove special characters
    input_text = input_text.lower()  # Convert to lowercase
    tokens = word_tokenize(input_text)
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stopwords.words('english')]
    return tokens  # Return the tokenized form

# Preprocess the manual input
manual_input_tokens = preprocess_manual_input(manual_input)

# Step 1: Vectorize the manual input using the trained TF-IDF vectorizer
manual_input_tfidf = tfidf.transform([' '.join(manual_input_tokens)])  # Join tokens back into a single string

# Step 2: Apply LSA (TruncatedSVD) to reduce dimensionality using the same SVD model
manual_input_lsa = lsa.transform(manual_input_tfidf)

# Step 3: Get the Word2Vec embedding for the manual input (same approach as during training)
manual_input_w2v = get_avg_w2v(manual_input_tokens, w2v_model, 100)

# Step 4: Combine the LSA-reduced TF-IDF features with Word2Vec embeddings
manual_input_combined = np.hstack([manual_input_lsa, manual_input_w2v.reshape(1, -1)])  # Reshape to match dimensions

# Step 5: Make a prediction using the trained XGBoost model
manual_prediction = xgb_model.predict(manual_input_combined)

# Print the result (1 for real news, 0 for fake news)
if manual_prediction == 1:
    print("The news is REAL.")
else:
    print("The news is FAKE.")

# Example manual input (this is the new tweet/news you want to test)
manual_input = "Narendra modi is india's prime minister."

# Preprocess the input (same preprocessing steps you applied to your dataset)
def preprocess_manual_input(input_text):
    # Apply the same preprocessing steps: remove URLs, punctuation, stopwords, etc.
    input_text = re.sub(r'http\S+', '', input_text)  # Remove URLs
    input_text = re.sub(r'\W', ' ', input_text)  # Remove special characters
    input_text = input_text.lower()  # Convert to lowercase
    tokens = word_tokenize(input_text)
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stopwords.words('english')]
    return tokens  # Return the tokenized form

# Preprocess the manual input
manual_input_tokens = preprocess_manual_input(manual_input)

# Step 1: Vectorize the manual input using the trained TF-IDF vectorizer
manual_input_tfidf = tfidf.transform([' '.join(manual_input_tokens)])  # Join tokens back into a single string

# Step 2: Apply LSA (TruncatedSVD) to reduce dimensionality using the same SVD model
manual_input_lsa = lsa.transform(manual_input_tfidf)

# Step 3: Get the Word2Vec embedding for the manual input (same approach as during training)
manual_input_w2v = get_avg_w2v(manual_input_tokens, w2v_model, 100)

# Step 4: Combine the LSA-reduced TF-IDF features with Word2Vec embeddings
manual_input_combined = np.hstack([manual_input_lsa, manual_input_w2v.reshape(1, -1)])  # Reshape to match dimensions

# Step 5: Make a prediction using the trained XGBoost model
manual_prediction = xgb_model.predict(manual_input_combined)

# Print the result (1 for real news, 0 for fake news)
if manual_prediction == 1:
    print("The news is REAL.")
else:
    print("The news is FAKE.")

import requests
import numpy as np

# Function to call Google Fact-Check API
def fact_check_google(query, api_key):
    url = f"https://factchecktools.googleapis.com/v1alpha1/claims:search?key={api_key}&query={query}"
    response = requests.get(url)
    if response.status_code == 200:
        return response.json()  # Return the JSON response
    else:
        return None

# Your API Key for Google Fact-Check API
api_key = 'AIzaSyBE3cauEJ_5rvIOY1itCAJRaUo8snUxe5A'

# Function to preprocess the manual input
def preprocess_manual_input(input_text):
    # Apply the same preprocessing steps: remove URLs, punctuation, stopwords, etc.
    input_text = re.sub(r'http\S+', '', input_text)  # Remove URLs
    input_text = re.sub(r'\W', ' ', input_text)  # Remove special characters
    input_text = input_text.lower()  # Convert to lowercase
    tokens = word_tokenize(input_text)
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stopwords.words('english')]
    return tokens  # Return the tokenized form

# Hybrid prediction function (model + fact-checking API)
def hybrid_prediction(manual_input, model, api_key):
    # Step 1: Preprocess the input and make the model prediction
    manual_input_tokens = preprocess_manual_input(manual_input)  # Preprocess input
    manual_input_tfidf = tfidf.transform([' '.join(manual_input_tokens)])  # TF-IDF vectorization
    manual_input_lsa = lsa.transform(manual_input_tfidf)  # Apply LSA (SVD)
    manual_input_w2v = get_avg_w2v(manual_input_tokens, w2v_model, 100)  # Get Word2Vec embedding

    # Combine LSA + Word2Vec embeddings
    manual_input_combined = np.hstack([manual_input_lsa, manual_input_w2v.reshape(1, -1)])

    # Model Prediction
    model_prediction = model.predict(manual_input_combined)

    # Step 2: Call the Google Fact-Check API
    fact_check_result = fact_check_google(manual_input, api_key)

    # Step 3: Analyze the fact-checking response
    if fact_check_result:
        claims = fact_check_result.get('claims', [])
        for claim in claims:
            claim_rating = claim['claimReview'][0]['textualRating']
            if "False" in claim_rating or "Pants on Fire" in claim_rating:
                print("Fact-Check API: The news is FAKE.")
                return "The news is FAKE."  # Override the model's prediction with API result

    # Step 4: Return model prediction if no fact-check results
    if model_prediction == 1:
        print("Model Prediction: The news is REAL.")
        return "The news is REAL."
    else:
        print("Model Prediction: The news is FAKE.")
        return "The news is FAKE."

# Example manual input (news/tweet to test)
manual_input = "Climate change is accelerating, with 2020 being one of the hottest years on record globally."

# Call the hybrid prediction function
hybrid_prediction(manual_input, xgb_model, api_key)

import requests
import numpy as np

# Function to call Google Fact-Check API
def fact_check_google(query, api_key):
    url = f"https://factchecktools.googleapis.com/v1alpha1/claims:search?key={api_key}&query={query}"
    response = requests.get(url)
    if response.status_code == 200:
        return response.json()  # Return the JSON response
    else:
        return None

# Your API Key for Google Fact-Check API
api_key = 'AIzaSyBE3cauEJ_5rvIOY1itCAJRaUo8snUxe5A'

# Function to preprocess the manual input
def preprocess_manual_input(input_text):
    # Apply the same preprocessing steps: remove URLs, punctuation, stopwords, etc.
    input_text = re.sub(r'http\S+', '', input_text)  # Remove URLs
    input_text = re.sub(r'\W', ' ', input_text)  # Remove special characters
    input_text = input_text.lower()  # Convert to lowercase
    tokens = word_tokenize(input_text)
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stopwords.words('english')]
    return tokens  # Return the tokenized form

# Hybrid prediction function (model + fact-checking API)
def hybrid_prediction(manual_input, model, api_key):
    # Step 1: Preprocess the input and make the model prediction
    manual_input_tokens = preprocess_manual_input(manual_input)  # Preprocess input
    manual_input_tfidf = tfidf.transform([' '.join(manual_input_tokens)])  # TF-IDF vectorization
    manual_input_lsa = lsa.transform(manual_input_tfidf)  # Apply LSA (SVD)
    manual_input_w2v = get_avg_w2v(manual_input_tokens, w2v_model, 100)  # Get Word2Vec embedding

    # Combine LSA + Word2Vec embeddings
    manual_input_combined = np.hstack([manual_input_lsa, manual_input_w2v.reshape(1, -1)])

    # Model Prediction
    model_prediction = model.predict(manual_input_combined)

    # Step 2: Call the Google Fact-Check API
    fact_check_result = fact_check_google(manual_input, api_key)

    # Step 3: Analyze the fact-checking response
    if fact_check_result:
        claims = fact_check_result.get('claims', [])
        for claim in claims:
            claim_rating = claim['claimReview'][0]['textualRating']
            if "False" in claim_rating or "Pants on Fire" in claim_rating:
                print("Fact-Check API: The news is FAKE.")
                return "The news is FAKE."  # Override the model's prediction with API result

    # Step 4: Return model prediction if no fact-check results
    if model_prediction == 1:
        print("Model Prediction: The news is REAL.")
        return "The news is REAL."
    else:
        print("Model Prediction: The news is FAKE.")
        return "The news is FAKE."

# Example manual input (news/tweet to test)
manual_input = "The moon landing was staged and faked by NASA in 1969"

# Call the hybrid prediction function
hybrid_prediction(manual_input, xgb_model, api_key)

import requests
import numpy as np

# Function to call Google Fact-Check API
def fact_check_google(query, api_key):
    url = f"https://factchecktools.googleapis.com/v1alpha1/claims:search?key={api_key}&query={query}"
    response = requests.get(url)
    if response.status_code == 200:
        return response.json()  # Return the JSON response
    else:
        return None

# Your API Key for Google Fact-Check API
api_key = 'AIzaSyBE3cauEJ_5rvIOY1itCAJRaUo8snUxe5A'

# Function to preprocess the manual input
def preprocess_manual_input(input_text):
    # Apply the same preprocessing steps: remove URLs, punctuation, stopwords, etc.
    input_text = re.sub(r'http\S+', '', input_text)  # Remove URLs
    input_text = re.sub(r'\W', ' ', input_text)  # Remove special characters
    input_text = input_text.lower()  # Convert to lowercase
    tokens = word_tokenize(input_text)
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stopwords.words('english')]
    return tokens  # Return the tokenized form

# Hybrid prediction function (model + fact-checking API)
def hybrid_prediction(manual_input, model, api_key):
    # Step 1: Preprocess the input and make the model prediction
    manual_input_tokens = preprocess_manual_input(manual_input)  # Preprocess input
    manual_input_tfidf = tfidf.transform([' '.join(manual_input_tokens)])  # TF-IDF vectorization
    manual_input_lsa = lsa.transform(manual_input_tfidf)  # Apply LSA (SVD)
    manual_input_w2v = get_avg_w2v(manual_input_tokens, w2v_model, 100)  # Get Word2Vec embedding

    # Combine LSA + Word2Vec embeddings
    manual_input_combined = np.hstack([manual_input_lsa, manual_input_w2v.reshape(1, -1)])

    # Model Prediction
    model_prediction = model.predict(manual_input_combined)

    # Step 2: Call the Google Fact-Check API
    fact_check_result = fact_check_google(manual_input, api_key)

    # Step 3: Analyze the fact-checking response
    if fact_check_result:
        claims = fact_check_result.get('claims', [])
        for claim in claims:
            claim_rating = claim['claimReview'][0]['textualRating']
            if "False" in claim_rating or "Pants on Fire" in claim_rating:
                print("Fact-Check API: The news is FAKE.")
                return "The news is FAKE."  # Override the model's prediction with API result

    # Step 4: Return model prediction if no fact-check results
    if model_prediction == 1:
        print("Model Prediction: The news is REAL.")
        return "The news is REAL."
    else:
        print("Model Prediction: The news is FAKE.")
        return "The news is FAKE."

# Example manual input (news/tweet to test)
manual_input = "Drinking bleach cures COVID-19."

# Call the hybrid prediction function
hybrid_prediction(manual_input, xgb_model, api_key)

import requests
import numpy as np

# Function to call Google Fact-Check API
def fact_check_google(query, api_key):
    url = f"https://factchecktools.googleapis.com/v1alpha1/claims:search?key={api_key}&query={query}"
    response = requests.get(url)
    if response.status_code == 200:
        return response.json()  # Return the JSON response
    else:
        return None

# Your API Key for Google Fact-Check API
api_key = 'AIzaSyBE3cauEJ_5rvIOY1itCAJRaUo8snUxe5A'

# Function to preprocess the manual input
def preprocess_manual_input(input_text):
    # Apply the same preprocessing steps: remove URLs, punctuation, stopwords, etc.
    input_text = re.sub(r'http\S+', '', input_text)  # Remove URLs
    input_text = re.sub(r'\W', ' ', input_text)  # Remove special characters
    input_text = input_text.lower()  # Convert to lowercase
    tokens = word_tokenize(input_text)
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stopwords.words('english')]
    return tokens  # Return the tokenized form

# Hybrid prediction function (model + fact-checking API)
def hybrid_prediction(manual_input, model, api_key):
    # Step 1: Preprocess the input and make the model prediction
    manual_input_tokens = preprocess_manual_input(manual_input)  # Preprocess input
    manual_input_tfidf = tfidf.transform([' '.join(manual_input_tokens)])  # TF-IDF vectorization
    manual_input_lsa = lsa.transform(manual_input_tfidf)  # Apply LSA (SVD)
    manual_input_w2v = get_avg_w2v(manual_input_tokens, w2v_model, 100)  # Get Word2Vec embedding

    # Combine LSA + Word2Vec embeddings
    manual_input_combined = np.hstack([manual_input_lsa, manual_input_w2v.reshape(1, -1)])

    # Model Prediction
    model_prediction = model.predict(manual_input_combined)

    # Step 2: Call the Google Fact-Check API
    fact_check_result = fact_check_google(manual_input, api_key)

    # Step 3: Analyze the fact-checking response
    if fact_check_result:
        claims = fact_check_result.get('claims', [])
        for claim in claims:
            claim_rating = claim['claimReview'][0]['textualRating']
            if "False" in claim_rating or "Pants on Fire" in claim_rating:
                print("Fact-Check API: The news is FAKE.")
                return "The news is FAKE."  # Override the model's prediction with API result

    # Step 4: Return model prediction if no fact-check results
    if model_prediction == 1:
        print("Model Prediction: The news is REAL.")
        return "The news is REAL."
    else:
        print("Model Prediction: The news is FAKE.")
        return "The news is FAKE."

# Example manual input (news/tweet to test)
manual_input = "Donald trump visited india in 2020"

# Call the hybrid prediction function
hybrid_prediction(manual_input, xgb_model, api_key)

import requests
import numpy as np

# Function to call Google Fact-Check API
def fact_check_google(query, api_key):
    url = f"https://factchecktools.googleapis.com/v1alpha1/claims:search?key={api_key}&query={query}"
    response = requests.get(url)
    if response.status_code == 200:
        return response.json()  # Return the JSON response
    else:
        return None

# Your API Key for Google Fact-Check API
api_key = 'AIzaSyBE3cauEJ_5rvIOY1itCAJRaUo8snUxe5A'

# Function to preprocess the manual input
def preprocess_manual_input(input_text):
    # Apply the same preprocessing steps: remove URLs, punctuation, stopwords, etc.
    input_text = re.sub(r'http\S+', '', input_text)  # Remove URLs
    input_text = re.sub(r'\W', ' ', input_text)  # Remove special characters
    input_text = input_text.lower()  # Convert to lowercase
    tokens = word_tokenize(input_text)
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stopwords.words('english')]
    return tokens  # Return the tokenized form

# Hybrid prediction function (model + fact-checking API)
def hybrid_prediction(manual_input, model, api_key):
    # Step 1: Preprocess the input and make the model prediction
    manual_input_tokens = preprocess_manual_input(manual_input)  # Preprocess input
    manual_input_tfidf = tfidf.transform([' '.join(manual_input_tokens)])  # TF-IDF vectorization
    manual_input_lsa = lsa.transform(manual_input_tfidf)  # Apply LSA (SVD)
    manual_input_w2v = get_avg_w2v(manual_input_tokens, w2v_model, 100)  # Get Word2Vec embedding

    # Combine LSA + Word2Vec embeddings
    manual_input_combined = np.hstack([manual_input_lsa, manual_input_w2v.reshape(1, -1)])

    # Model Prediction
    model_prediction = model.predict(manual_input_combined)

    # Step 2: Call the Google Fact-Check API
    fact_check_result = fact_check_google(manual_input, api_key)

    # Step 3: Analyze the fact-checking response
    if fact_check_result:
        claims = fact_check_result.get('claims', [])
        for claim in claims:
            claim_rating = claim['claimReview'][0]['textualRating']
            if "False" in claim_rating or "Pants on Fire" in claim_rating:
                print("Fact-Check API: The news is FAKE.")
                return "The news is FAKE."  # Override the model's prediction with API result

    # Step 4: Return model prediction if no fact-check results
    if model_prediction == 1:
        print("Model Prediction: The news is REAL.")
        return "The news is REAL."
    else:
        print("Model Prediction: The news is FAKE.")
        return "The news is FAKE."

# Example manual input (news/tweet to test)
manual_input = "COVID-19 was created in a laboratory as a bioweapon."

# Call the hybrid prediction function
hybrid_prediction(manual_input, xgb_model, api_key)